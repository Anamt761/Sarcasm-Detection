{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqQ5J4SMNE0p"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-onXHy22e5s7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "import tqdm\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYhTkiY3Zfze"
      },
      "outputs": [],
      "source": [
        "# Function to read JSON lines file\n",
        "def read_json_lines(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWtjmBKINNjO"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "file1_path = '/content/drive/MyDrive/Sarcasm_Headlines_Dataset.json'\n",
        "file2_path = '/content/drive/MyDrive/Sarcasm_Headlines_Dataset_v2.json'\n",
        "\n",
        "df1 = pd.read_json(file1_path, lines=True)\n",
        "df2 = pd.read_json(file2_path, lines=True)\n",
        "\n",
        "# Concatenate the datasets\n",
        "df = pd.concat([df1, df2], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Separate sarcastic and non-sarcastic headlines\n",
        "sarcastic_headlines = df[df['is_sarcastic'] == 1]['headline']\n",
        "non_sarcastic_headlines = df[df['is_sarcastic'] == 0]['headline']\n",
        "\n",
        "# Extract three examples each\n",
        "sarcastic_examples = sarcastic_headlines.sample(10, random_state=42).tolist()\n",
        "non_sarcastic_examples = non_sarcastic_headlines.sample(10, random_state=42).tolist()\n",
        "\n",
        "print(\"Sarcastic Headlines:\")\n",
        "for i, headline in enumerate(sarcastic_examples, 1):\n",
        "    print(f\"{i}. {headline}\")\n",
        "\n",
        "print(\"\\nNon-Sarcastic Headlines:\")\n",
        "for i, headline in enumerate(non_sarcastic_examples, 1):\n",
        "    print(f\"{i}. {headline}\")\n"
      ],
      "metadata": {
        "id": "xROdllabNmaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extracting 3 sarcastic sentences\n",
        "sarcastic_samples = df[df['is_sarcastic'] == 1]['headline'].sample(3).tolist()\n",
        "\n",
        "# Extracting 3 non-sarcastic sentences\n",
        "non_sarcastic_samples = df[df['is_sarcastic'] == 0]['headline'].sample(3).tolist()\n",
        "\n",
        "# Creating a DataFrame to display the samples\n",
        "data = {\n",
        "    'Sentence': sarcastic_samples + non_sarcastic_samples,\n",
        "    'Expression': ['sarcastic'] * 3 + ['non-sarcastic'] * 3\n",
        "}\n",
        "\n",
        "df_samples = pd.DataFrame(data)\n",
        "\n",
        "# Displaying the table\n",
        "print(df_samples)\n"
      ],
      "metadata": {
        "id": "FAvFdU1LU54P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dBvzAGl9EFm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Separate sarcastic and non-sarcastic headlines\n",
        "sarcastic_headlines = df[df['is_sarcastic'] == 1]['headline']\n",
        "non_sarcastic_headlines = df[df['is_sarcastic'] == 0]['headline']\n",
        "\n",
        "# Create word clouds\n",
        "sarcastic_text = ' '.join(sarcastic_headlines)\n",
        "non_sarcastic_text = ' '.join(non_sarcastic_headlines)\n",
        "\n",
        "sarcastic_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sarcastic_text)\n",
        "non_sarcastic_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(non_sarcastic_text)\n",
        "\n",
        "# Plot the word clouds\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(sarcastic_wordcloud, interpolation='bilinear')\n",
        "plt.title('Sarcastic Headlines Word Cloud')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(non_sarcastic_wordcloud, interpolation='bilinear')\n",
        "plt.title('Non-Sarcastic Headlines Word Cloud')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XvWVkPpKRrg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6gDM7goOczu"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KouKRsfOFJ4"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "df.drop(columns=['article_link'], inplace=True)  # Drop the 'article_link' column\n",
        "df.dropna(inplace=True)  # Drop any rows with missing values\n",
        "df['headline'] = df['headline'].str.lower()  # Convert text to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEtOyQMTOjPk"
      },
      "outputs": [],
      "source": [
        "# Basic text preprocessing\n",
        "import re\n",
        "# Initialize stopwords, lemmatizer, and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~]', '', text)\n",
        "\n",
        "        # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Stem\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Rejoin tokens into a single string\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "df['headline'] = df['headline'].apply(preprocess_text)\n",
        "\n",
        "# Check for any missing values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Apply preprocessing to the 'headline' column\n",
        "df['headline'] = df['headline'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows after preprocessing\n",
        "print(\"\\nAfter Preprocessing:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOH2e9hKfFft"
      },
      "source": [
        "### ***EDA***\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf_BMhkefPFH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Separate sarcastic and non-sarcastic headlines\n",
        "sarcastic_headlines = df[df['is_sarcastic'] == 1]['headline']\n",
        "non_sarcastic_headlines = df[df['is_sarcastic'] == 0]['headline']\n",
        "\n",
        "# Create word clouds\n",
        "sarcastic_text = ' '.join(sarcastic_headlines)\n",
        "non_sarcastic_text = ' '.join(non_sarcastic_headlines)\n",
        "\n",
        "sarcastic_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sarcastic_text)\n",
        "non_sarcastic_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(non_sarcastic_text)\n",
        "\n",
        "# Plot the word clouds\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(sarcastic_wordcloud, interpolation='bilinear')\n",
        "plt.title('a) Sarcastic Headlines Word Cloud')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(non_sarcastic_wordcloud, interpolation='bilinear')\n",
        "plt.title('b) Non-Sarcastic Headlines Word Cloud')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pHqMRHl_SAuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Calculate the length of each headline\n",
        "df['headline_length'] = df['headline'].apply(len)\n",
        "\n",
        "# Separate the lengths based on the target variable\n",
        "sarcastic_lengths = df[df['is_sarcastic'] == 1]['headline_length']\n",
        "non_sarcastic_lengths = df[df['is_sarcastic'] == 0]['headline_length']\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "plt.hist(non_sarcastic_lengths, bins=30, alpha=0.5, label='Non-Sarcastic', color='blue', edgecolor='black')\n",
        "plt.hist(sarcastic_lengths, bins=30, alpha=0.5, label='Sarcastic', color='red', edgecolor='black')\n",
        "\n",
        "plt.xlabel('Headline Length')\n",
        "plt.ylabel('Number of Headlines')\n",
        "plt.title('Histogram of Headline Lengths by Sarcasm')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u2h1dHjnSF5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the length of each headline\n",
        "df['headline_length'] = df['headline'].apply(len)\n",
        "\n",
        "# Separate the lengths based on the target variable\n",
        "sarcastic_lengths = df[df['is_sarcastic'] == 1]['headline_length']\n",
        "non_sarcastic_lengths = df[df['is_sarcastic'] == 0]['headline_length']\n",
        "\n",
        "# Calculate the mean lengths\n",
        "mean_sarcastic_length = sarcastic_lengths.mean()\n",
        "mean_non_sarcastic_length = non_sarcastic_lengths.mean()\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Histogram for non-sarcastic headlines\n",
        "plt.hist(non_sarcastic_lengths, bins=30, alpha=0.5, label='Non-Sarcastic', color='blue', edgecolor='black')\n",
        "# Plot the mean line for non-sarcastic\n",
        "plt.axvline(mean_non_sarcastic_length, color='blue', linestyle='dashed', linewidth=2)\n",
        "plt.text(mean_non_sarcastic_length + 2, plt.ylim()[1] * 0.9, f'Mean: {mean_non_sarcastic_length:.2f}', color='blue')\n",
        "\n",
        "# Histogram for sarcastic headlines\n",
        "plt.hist(sarcastic_lengths, bins=30, alpha=0.5, label='Sarcastic', color='red', edgecolor='black')\n",
        "# Plot the mean line for sarcastic\n",
        "plt.axvline(mean_sarcastic_length, color='red', linestyle='dashed', linewidth=2)\n",
        "plt.text(mean_sarcastic_length + 2, plt.ylim()[1] * 0.9, f'Mean: {mean_sarcastic_length:.2f}', color='red')\n",
        "\n",
        "plt.xlabel('Headline Length')\n",
        "plt.ylabel('Number of Headlines')\n",
        "plt.title('Histogram of Headline Lengths by Sarcasm with Mean Lengths')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yNqlRApvScil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSdq2oMHfUYe"
      },
      "outputs": [],
      "source": [
        "# Counts of sarcastic and non-sarcastic headlines\n",
        "count_sarcastic = df['is_sarcastic'].value_counts()\n",
        "\n",
        "# Import seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Bar plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=count_sarcastic.index, y=count_sarcastic.values, palette='viridis')\n",
        "plt.title('Counts of Sarcastic and Non-Sarcastic Headlines')\n",
        "plt.xlabel('Sarcasm')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['Non-Sarcastic', 'Sarcastic'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is already loaded with concatenated datasets\n",
        "# Extracting 3 sarcastic sentences\n",
        "sarcastic_samples = df[df['is_sarcastic'] == 1]['headline'].sample(3).tolist()\n",
        "\n",
        "# Extracting 3 non-sarcastic sentences\n",
        "non_sarcastic_samples = df[df['is_sarcastic'] == 0]['headline'].sample(3).tolist()\n",
        "\n",
        "# Creating a DataFrame to display the samples\n",
        "data = {\n",
        "    'Sentence': sarcastic_samples + non_sarcastic_samples,\n",
        "    'Expression': ['sarcastic'] * 3 + ['non-sarcastic'] * 3\n",
        "}\n",
        "\n",
        "df_samples = pd.DataFrame(data)\n",
        "\n",
        "# Displaying the table\n",
        "print(df_samples)\n"
      ],
      "metadata": {
        "id": "z2uBjmESUwrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50uXJu5HfsLJ"
      },
      "outputs": [],
      "source": [
        "# Pie chart for target variable distribution\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.pie(count_sarcastic, labels=['Non-Sarcastic', 'Sarcastic'], autopct='%1.1f%%', startangle=140, colors=['lightblue', 'lightgreen'])\n",
        "plt.title('Target Variable Distribution')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from collections import Counter # Import the Counter class\n"
      ],
      "metadata": {
        "id": "OkeVs7l0dwso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDuDlCINfxO8"
      },
      "outputs": [],
      "source": [
        "# Tokenize the headlines\n",
        "df['tokens'] = df['headline'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Flatten the list of tokens and count word frequencies\n",
        "all_words = [word for tokens in df['tokens'] for word in tokens]\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "# Get the 20 most common words\n",
        "most_common_words = word_freq.most_common(20)\n",
        "\n",
        "# Bar plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=[word[0] for word in most_common_words], y=[word[1] for word in most_common_words], palette='viridis')\n",
        "plt.title('Most Used Words')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZvkiaaZf2e_"
      },
      "outputs": [],
      "source": [
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('Word Cloud of Headlines')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yok4qcwMPDUy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train-test split\n",
        "X = df['headline']\n",
        "y = df['is_sarcastic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***ML+POS***"
      ],
      "metadata": {
        "id": "tOHnonkC-dEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD7oZHz4bEj7"
      },
      "outputs": [],
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'LightGBM': lgb.LGBMClassifier()\n",
        "}\n",
        "\n",
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, sarcasm):\n",
        "    print(f\"Training models for {sarcasm}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {sarcasm}...\")\n",
        "        # Initialize the TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        # Convert POS-tagged text to numerical features for training and testing data\n",
        "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "        X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_vectorized, y_train)\n",
        "        y_pred = model.predict(X_test_vectorized)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {sarcasm}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {sarcasm}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reZRlu1qPMIO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# POS tagging feature extractor\n",
        "class PosTagTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts):\n",
        "        return [' '.join([tag for word, tag in pos_tag(word_tokenize(text))]) for text in texts]\n",
        "\n",
        "# Define the vectorizers\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "pos_tag_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Combine the features\n",
        "combined_features = FeatureUnion([\n",
        "    ('tfidf', tfidf_vectorizer),\n",
        "    ('ngram', ngram_vectorizer),\n",
        "    ('pos_tag', Pipeline([\n",
        "        ('pos_transform', PosTagTransformer()),\n",
        "        ('tfidf', pos_tag_vectorizer)\n",
        "    ]))\n",
        "])\n",
        "\n",
        "# Apply combined features\n",
        "X_train_features = combined_features.fit_transform(X_train)\n",
        "X_test_features = combined_features.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOrWOdfBb3gm"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_model(X_train, X_test, y_train, y_test, \"Sarcasm Detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DnLdAvvHPSfu"
      },
      "outputs": [],
      "source": [
        "# Train the SVM model\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_model.predict(X_test_features)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtEb1irfTrUw"
      },
      "outputs": [],
      "source": [
        "# POS tagging feature extractor\n",
        "class PosTagTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts):\n",
        "        return [' '.join([tag for word, tag in pos_tag(word_tokenize(text))]) for text in texts]\n",
        "\n",
        "# Apply POS tagging and TF-IDF\n",
        "pos_tag_vectorizer = TfidfVectorizer()\n",
        "pos_transformer = PosTagTransformer()\n",
        "X_train_pos = pos_transformer.transform(X_train)\n",
        "X_test_pos = pos_transformer.transform(X_test)\n",
        "\n",
        "X_train_pos_tfidf = pos_tag_vectorizer.fit_transform(X_train_pos)\n",
        "X_test_pos_tfidf = pos_tag_vectorizer.transform(X_test_pos)\n",
        "\n",
        "# Train SVM with POS tagging features\n",
        "svm_pos = SVC(kernel='linear', random_state=42)\n",
        "svm_pos.fit(X_train_pos_tfidf, y_train)\n",
        "y_pred_pos = svm_pos.predict(X_test_pos_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix_pos = confusion_matrix(y_test, y_pred_pos)\n",
        "class_report_pos = classification_report(y_test, y_pred_pos)\n",
        "\n",
        "print(\"Confusion Matrix (POS Tagging):\\n\", conf_matrix_pos)\n",
        "print(\"\\nClassification Report (POS Tagging):\\n\", class_report_pos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***N-GRAM***"
      ],
      "metadata": {
        "id": "xz0pUYb9wvD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNQb33yDTxu1"
      },
      "outputs": [],
      "source": [
        "# N-gram Vectorizer\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM with N-gram features\n",
        "svm_ngram = SVC(kernel='linear', random_state=42)\n",
        "svm_ngram.fit(X_train_ngram, y_train)\n",
        "y_pred_ngram = svm_ngram.predict(X_test_ngram)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix_ngram = confusion_matrix(y_test, y_pred_ngram)\n",
        "class_report_ngram = classification_report(y_test, y_pred_ngram)\n",
        "\n",
        "print(\"Confusion Matrix (N-grams):\\n\", conf_matrix_ngram)\n",
        "print(\"\\nClassification Report (N-grams):\\n\", class_report_ngram)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2WWZupdehQQ"
      },
      "source": [
        "### ***TF-IDF***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B80dyMkwfQ7L"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining and evaluating {model_name} with TF-IDF features...\")\n",
        "    train_evaluate_model(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TF-IDF')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvgbZrGc_e6D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Initialize a figure for the combined ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Initialize arrays to store combined false positive rates and true positive rates\n",
        "all_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = 0.0\n",
        "\n",
        "\n",
        "# Plot ROC curve for each classifier and calculate the mean true positive rate\n",
        "for name, model in models.items():\n",
        "    # Fit the model\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Get scores (decision function output) on the test set\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        scores = model.decision_function(X_test_tfidf)\n",
        "    else:\n",
        "        scores = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Convert scores into probabilities\n",
        "    y_pred_proba = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "    # Compute ROC curve and ROC area for Sarcasm Detection - NGRAM\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve for the model\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Calculate mean true positive rate\n",
        "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "\n",
        "# Calculate the mean true positive rate across all classifiers\n",
        "mean_tpr /= len(models)\n",
        "mean_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "    # Plot the combined ROC curve\n",
        "plt.plot(all_fpr, mean_tpr, color='black', linestyle='--', lw=2, label=f'Combined ROC (AUC = {mean_auc:.2f})')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Combined Receiver Operating Characteristic (ROC) Curve for TF-IDF')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oC5DHOYfF_r"
      },
      "source": [
        "### ***POS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0do5eYhgPQ0"
      },
      "outputs": [],
      "source": [
        "class PosTagTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts):\n",
        "        return [' '.join([tag for word, tag in pos_tag(word_tokenize(text))]) for text in texts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hfst8gk8fHpu"
      },
      "outputs": [],
      "source": [
        "# Apply POS tagging and TF-IDF\n",
        "pos_tag_vectorizer = TfidfVectorizer()\n",
        "pos_transformer = PosTagTransformer()\n",
        "X_train_pos = pos_transformer.transform(X_train)\n",
        "X_test_pos = pos_transformer.transform(X_test)\n",
        "\n",
        "X_train_pos_tfidf = pos_tag_vectorizer.fit_transform(X_train_pos)\n",
        "X_test_pos_tfidf = pos_tag_vectorizer.transform(X_test_pos)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining and evaluating {model_name} with POS Tagging features...\")\n",
        "    train_evaluate_model(model, X_train_pos_tfidf, X_test_pos_tfidf, y_train, y_test, 'POS Tagging')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5hqDPazCs6E"
      },
      "outputs": [],
      "source": [
        "# POS Tagging Transformer\n",
        "class PosTagTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts):\n",
        "        return [' '.join([tag for word, tag in pos_tag(word_tokenize(text))]) for text in texts]\n",
        "\n",
        "# Apply POS tagging\n",
        "pos_transformer = PosTagTransformer()\n",
        "X_train_pos = pos_transformer.transform(X_train)\n",
        "X_test_pos = pos_transformer.transform(X_test)\n",
        "\n",
        "# TF-IDF Vectorizer for POS tags\n",
        "pos_tag_vectorizer = TfidfVectorizer()\n",
        "X_train_pos_tfidf = pos_tag_vectorizer.fit_transform(X_train_pos)\n",
        "X_test_pos_tfidf = pos_tag_vectorizer.transform(X_test_pos)\n",
        "\n",
        "# Initialize a figure for the combined ROC curve for POS Tagging features\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Initialize arrays to store combined false positive rates and true positive rates\n",
        "all_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = 0.0\n",
        "\n",
        "# Plot ROC curve for each classifier and calculate the mean true positive rate for POS Tagging features\n",
        "for name, model in models.items():\n",
        "    # Fit the model\n",
        "    model.fit(X_train_pos_tfidf, y_train)\n",
        "\n",
        "    # Get scores (decision function output) on the test set\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        scores = model.decision_function(X_test_pos_tfidf)\n",
        "    else:\n",
        "        scores = model.predict(X_test_pos_tfidf)\n",
        "\n",
        "    # Convert scores into probabilities\n",
        "    y_pred_proba = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "    # Compute ROC curve and ROC area for Sarcasm Detection - POS Tagging\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve for the model\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Calculate mean true positive rate\n",
        "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "# Calculate the mean true positive rate across all classifiers\n",
        "mean_tpr /= len(models)\n",
        "mean_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "# Plot the combined ROC curve\n",
        "plt.plot(all_fpr, mean_tpr, color='black', linestyle='--', lw=2, label=f'Combined ROC (AUC = {mean_auc:.2f})')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Combined Receiver Operating Characteristic (ROC) Curve for POS Tagging')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WERHjIuDiDyK"
      },
      "outputs": [],
      "source": [
        "# N-gram Vectorizer\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining and evaluating {model_name} with N-gram features...\")\n",
        "    train_evaluate_model(model, X_train_ngram, X_test_ngram, y_train, y_test, 'N-grams')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llsjvafxlECm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "# Initialize a figure for the combined ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Initialize arrays to store combined false positive rates and true positive rates\n",
        "all_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = 0.0\n",
        "\n",
        "\n",
        "# Plot ROC curve for each classifier and calculate the mean true positive rate\n",
        "for name, model in models.items():\n",
        "    # Fit the model\n",
        "    model.fit(X_train_ngram, y_train)\n",
        "\n",
        "    # Get scores (decision function output) on the test set\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        scores = model.decision_function(X_test_ngram)\n",
        "    else:\n",
        "        scores = model.predict(X_test_ngram)\n",
        "\n",
        "    # Convert scores into probabilities\n",
        "    y_pred_proba = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "    # Compute ROC curve and ROC area for Sarcasm Detection - NGRAM\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve for the model\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Calculate mean true positive rate\n",
        "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "\n",
        "# Calculate the mean true positive rate across all classifiers\n",
        "mean_tpr /= len(models)\n",
        "mean_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "    # Plot the combined ROC curve\n",
        "plt.plot(all_fpr, mean_tpr, color='black', linestyle='--', lw=2, label=f'Combined ROC (AUC = {mean_auc:.2f})')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Combined Receiver Operating Characteristic (ROC) Curve for N-Gram')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'LightGBM': lgb.LGBMClassifier()\n",
        "}\n",
        "\n",
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, sarcasm):\n",
        "    print(f\"Training models for {sarcasm}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {sarcasm}...\")\n",
        "        # Initialize the TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        # Convert POS-tagged text to numerical features for training and testing data\n",
        "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "        X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_vectorized, y_train)\n",
        "        y_pred = model.predict(X_test_vectorized)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {sarcasm}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {sarcasm}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")\n",
        "\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining and evaluating {model_name} with TF-IDF features...\")\n",
        "train_and_evaluate_model(X_train, X_test, y_train, y_test, \"TF-IDF\")\n"
      ],
      "metadata": {
        "id": "gNWuVa_KxcSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIJSVrkDHu10"
      },
      "source": [
        "### ***Deep Models***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzk0Y-gpHxLk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GRU, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models.fasttext import FastText\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout\n",
        "from torchtext.vocab import GloVe\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdLo2fyEI4kl"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding\n",
        "maxlen = 100  # You can adjust this value\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HGC7dSfJBbs"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=[nltk.word_tokenize(text) for text in X_train], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word_vectors = w2v_model.wv\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word_vectors:\n",
        "        embedding_matrix[i] = word_vectors[word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PWjxA-g_JEdn"
      },
      "outputs": [],
      "source": [
        "# Load the GloVe model\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Create an embedding matrix for Glove\n",
        "embedding_matrix_glove = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = glove[word]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_glove[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG560cNEEvhU"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG8CydgsD0Dc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D, Embedding, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Prepare data\n",
        "X = df['headline'].values\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y = to_categorical(y)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Sentence Transformer model\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Generate sentence embeddings\n",
        "X_train_embeddings = sbert_model.encode(X_train)\n",
        "X_test_embeddings = sbert_model.encode(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdDELpmuJk5g"
      },
      "outputs": [],
      "source": [
        "# Define model architectures using sentence embeddings\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_bilstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))\n",
        "    model.add(Bidirectional(LSTM(128)))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(128, 5, activation='relu', input_shape=input_shape))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(128, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(SimpleRNN(128))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5CtTlODJmWJ"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate models\n",
        "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "    print(classification_report(y_test_classes, y_pred_classes))\n",
        "    cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK0woGyrJstt"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of models\n",
        "models = {\n",
        "    \"LSTM\": create_lstm_model((X_train_embeddings.shape[1], 1)),\n",
        "    \"Bi-LSTM\": create_bilstm_model((X_train_embeddings.shape[1], 1)),\n",
        "    \"CNN\": create_cnn_model((X_train_embeddings.shape[1], 1)),\n",
        "    \"RNN\": create_rnn_model((X_train_embeddings.shape[1], 1))\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EGvKsotIsko"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    trained_model = train_and_evaluate_model(model, X_train_embeddings, y_train, X_test_embeddings, y_test)\n",
        "    results[name] = trained_model\n",
        "\n",
        "# Print results\n",
        "for name, result in results.items():\n",
        "    print(f\"{name} model trained and evaluated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR4nxB0qG-ji"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Convert headlines to sentence embeddings\n",
        "X_embeddings = sbert_model.encode(df['headline'].tolist())\n",
        "\n",
        "# Continue with the rest of your preprocessing and model training steps\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input shape\n",
        "input_shape = (X_train_embeddings.shape[1], 1)\n",
        "\n",
        "# Reshape data for RNN/CNN models\n",
        "X_train_embeddings_reshaped = np.expand_dims(X_train_embeddings, axis=2)\n",
        "X_test_embeddings_reshaped = np.expand_dims(X_test_embeddings, axis=2)"
      ],
      "metadata": {
        "id": "tuN86A_8xh6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model, X_train_pad, y_train, X_test_pad, y_test):\n",
        "    model.fit(X_train_pad, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "HBSTDdXeySCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec + LSTM\n",
        "print(\"Word2Vec + LSTM\")\n",
        "w2v_lstm_model = create_lstm_model(vocab_size, embedding_matrix)\n",
        "train_and_evaluate_model(w2v_lstm_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "XkeXWj8My2aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe + LSTM\n",
        "print(\"GloVe + LSTM\")\n",
        "glove_lstm_model = create_lstm_model(vocab_size, embedding_matrix_glove)\n",
        "train_and_evaluate_model(glove_lstm_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "o-Zfq7MXy9Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText + LSTM\n",
        "print(\"FastText + LSTM\")\n",
        "fasttext_lstm_model = create_lstm_model(vocab_size, embedding_matrix_fasttext)\n",
        "train_and_evaluate_model(fasttext_lstm_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "vJIPBVkQy41B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word2Vec + Bi-LSTM\")\n",
        "w2v_bilstm_model = create_lstm_model(vocab_size, embedding_matrix)\n",
        "train_and_evaluate_model(w2v_bilstm_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "c4KIcWe0zCF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe + Bi-LSTM\n",
        "print(\"GloVe + Bi-LSTM\")\n",
        "glove_bilstm_model = create_bilstm_model(vocab_size, embedding_matrix_glove)\n",
        "train_and_evaluate_model(glove_bilstm_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "Qn1gtq7wzI9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText + Bi-LSTM\n",
        "print(\"FastText + Bi-LSTM\")\n",
        "fasttext_bilstm_model = create_bilstm_model(vocab_size, embedding_matrix_fasttext)\n",
        "train_and_evaluate_model(fasttext_bilstm_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "cBybi5npzLnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec + CNN\n",
        "print(\"Word2Vec + CNN\")\n",
        "w2v_cnn_model = create_cnn_model(vocab_size, embedding_matrix)\n",
        "train_and_evaluate_model(w2v_cnn_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "e1pDMDcUzQq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe + CNN\n",
        "print(\"GloVe + CNN\")\n",
        "glove_cnn_model = create_cnn_model(vocab_size, embedding_matrix_glove)\n",
        "train_and_evaluate_model(glove_cnn_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "Y8wNJquCzTn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText + CNN\n",
        "print(\"FastText + CNN\")\n",
        "fasttext_cnn_model = create_cnn_model(vocab_size, embedding_matrix_fasttext)\n",
        "train_and_evaluate_model(fasttext_cnn_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "zuTzROKnzXYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec + RNN\n",
        "print(\"Word2Vec + RNN\")\n",
        "w2v_rnn_model = create_rnn_model(vocab_size, embedding_matrix)\n",
        "train_and_evaluate_model(w2v_rnn_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "Fjz_Pzrkzaul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe + RNN\n",
        "print(\"GloVe + RNN\")\n",
        "glove_rnn_model = create_rnn_model(vocab_size, embedding_matrix_glove)\n",
        "train_and_evaluate_model(glove_rnn_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "MFOIWpS3zdi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText + RNN\n",
        "print(\"FastText + RNN\")\n",
        "fasttext_rnn_model = create_rnn_model(vocab_size, embedding_matrix_fasttext)\n",
        "train_and_evaluate_model(fasttext_rnn_model, X_train_pad, y_train, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "GaOxx31Kzg4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model with Sentence-BERT\n",
        "def create_lstm_model_sbert(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, input_shape=(input_shape,)))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Bi-LSTM Model with Sentence-BERT\n",
        "def create_bilstm_model_sbert(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, input_shape=(input_shape,))))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# CNN Model with Sentence-BERT\n",
        "def create_CNN_model_sbert(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(CNN(100, dropout=0.2, recurrent_dropout=0.2, input_shape=(input_shape,)))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "    # RNN Model with Sentence-BERT\n",
        "def create_RNN_model_sbert(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(RNN(100, dropout=0.2, recurrent_dropout=0.2, input_shape=(input_shape,)))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "JCTguTU80ABw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the Sentence-BERT embeddings to add the time_steps dimension\n",
        "X_train_embeddings = np.expand_dims(X_train_embeddings, axis=1)\n",
        "X_test_embeddings = np.expand_dims(X_test_embeddings, axis=1)\n",
        "\n",
        "# Now input_shape should include the time_steps dimension\n",
        "input_shape = (X_train_embeddings.shape[1], X_train_embeddings.shape[2])\n",
        "\n",
        "# Train and evaluate LSTM with Sentence-BERT\n",
        "lstm_model_sbert = create_lstm_model_sbert(input_shape)\n",
        "lstm_model_sbert.fit(X_train_embeddings, y_train, epochs=35, batch_size=64, validation_data=(X_test_embeddings, y_test), verbose=2)\n",
        "\n",
        "# Train and evaluate Bi-LSTM with Sentence-BERT\n",
        "bilstm_model_sbert = create_bilstm_model_sbert(input_shape)\n",
        "bilstm_model_sbert.fit(X_train_embeddings, y_train, epochs=35, batch_size=64, validation_data=(X_test_embeddings, y_test), verbose=2)\n",
        "\n",
        "# Train and evaluate CNN with Sentence-BERT\n",
        "CNN_model_sbert = create_CNN_model_sbert(input_shape)\n",
        "CNN_model_sbert.fit(X_train_embeddings, y_train, epochs=35, batch_size=64, validation_data=(X_test_embeddings, y_test), verbose=2)\n",
        "\n",
        "# Train and evaluate RNN with Sentence-BERT\n",
        "RNN_model_sbert = create_RNN_model_sbert(input_shape)\n",
        "RNN_model_sbert.fit(X_train_embeddings, y_train, epochs=35, batch_size=64, validation_data=(X_test_embeddings, y_test), verbose=2)\n"
      ],
      "metadata": {
        "id": "sPxV6BX90Nx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LSTM with Sentence-BERT Classification Report:\\n\", classification_report(y_true, y_pred_lstm_sbert))\n"
      ],
      "metadata": {
        "id": "hGIGx84o0jQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bi-LSTM with Sentence-BERT Classification Report:\\n\", classification_report(y_true, y_pred_bilstm_sbert))\n"
      ],
      "metadata": {
        "id": "6iLubfxn0h2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CNN with Sentence-BERT Classification Report:\\n\", classification_report(y_true, y_pred_CNN_sbert))\n"
      ],
      "metadata": {
        "id": "Kz1eVqmv0kmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RNN with Sentence-BERT Classification Report:\\n\", classification_report(y_true, y_pred_RNN_sbert))\n"
      ],
      "metadata": {
        "id": "cwjdkHgK0m0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}